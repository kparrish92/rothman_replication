delta = 0,
sd = 27,
low_eqbound = -.4,
high_eqbound = .4,
alpha = .05,
power = .8,
type = "two.sample"
)
# ------------------------------------------------------
# Date 11/9/23
# This script simulates data for the purpose of determining
# the needed sample size
# -------------------------------------------------------
# Code for a two sample power analysis that runs both a t.test and tost
#number of iterations
k = 1000
temp_df = matrix(nrow = k, ncol = 2)
outer_df = matrix(nrow = 7, ncol = 3)
sizes = c(60,70,80,90,100,110,120)
for(thisSize in 1:nrow(outer_df)){
for(thisRun in 1:k){
mv1 = rnorm(95, m = 3.2, sd = 1.3)
mv2 = rnorm(115, m = 3.2, sd = 1.3)
tost = TOSTER::TOSTtwo(m1 = mean(mv1), m2 = mean(mv2), sd1 = sd(mv1), sd2 = sd(mv2),
n1 = sizes[thisSize], n2 = sizes[thisSize], low_eqbound_d = -.4, high_eqbound_d = .4, alpha = .05,
plot = TRUE, verbose = TRUE)
t_test_df = t.test(mv1, mv2, paired = FALSE)
temp_df[thisRun, 1] = pmax(tost$TOST_p1, tost$TOST_p2)
temp_df[thisRun, 2] = t_test_df$p.value
temp_df = as.data.frame(temp_df)
}
outer_df[thisSize, 1] = sizes[thisSize]
outer_df[thisSize, 2] = sum(temp_df$V1 < .05) # tost positives
outer_df[thisSize, 3] = sum(temp_df$V2 < .05) # (false) t-test positives
}
View(outer_df)
as.data.frame(outer_df)
source("~/Documents/GitHub/rothman_replication/scripts/99b_sample_size_justification.R", echo=TRUE)
as.data.frame(outer_df) %>%
rename(V1 == "Sample Size")
# ------------------------------------------------------
# Date 11/9/23
# This script simulates data for the purpose of determining
# the needed sample size
# -------------------------------------------------------
# Code for a two sample power analysis that runs both a t.test and tost
#number of iterations
k = 1000
temp_df = matrix(nrow = k, ncol = 2)
outer_df = matrix(nrow = 7, ncol = 3)
sizes = c(60,70,80,90,100,110,120)
for(thisSize in 1:nrow(outer_df)){
for(thisRun in 1:k){
mv1 = rnorm(95, m = 3.2, sd = 1.3)
mv2 = rnorm(115, m = 3.2, sd = 1.3)
tost = TOSTER::TOSTtwo(m1 = mean(mv1), m2 = mean(mv2), sd1 = sd(mv1), sd2 = sd(mv2),
n1 = sizes[thisSize], n2 = sizes[thisSize], low_eqbound_d = -.4, high_eqbound_d = .4, alpha = .05,
plot = TRUE, verbose = TRUE)
t_test_df = t.test(mv1, mv2, paired = FALSE)
temp_df[thisRun, 1] = pmax(tost$TOST_p1, tost$TOST_p2)
temp_df[thisRun, 2] = t_test_df$p.value
temp_df = as.data.frame(temp_df)
}
outer_df[thisSize, 1] = sizes[thisSize]
outer_df[thisSize, 2] = sum(temp_df$V1 < .05) # tost positives
outer_df[thisSize, 3] = sum(temp_df$V2 < .05) # (false) t-test positives
}
# tidy results of loop to make the output readable
as.data.frame(outer_df) %>%
rename(V1 == "Sample Size")
sample_needed = as.data.frame(outer_df) %>%
rename(V1 == "Sample Size")
sample_needed = as.data.frame(outer_df) %>%
rename("Sample Size" == V1)
sample_needed = as.data.frame(outer_df) %>%
rename("Sample Size" = V1)
View(sample_needed)
sample_needed = as.data.frame(outer_df) %>%
rename("Sample Size" = V1) %>%
mutate(V2 = V2/1000)
sample_needed = as.data.frame(outer_df) %>%
rename("Sample Size" = V1) %>%
mutate(V2 = V2/1000) %>%
mutate(V3 = V3/1000) %>%
rename("Pct Positive TOST" = V2) %>%
rename("Pct Positive T-test" = V3) %>%
sample_needed = as.data.frame(outer_df) %>%
rename("Sample Size" = V1) %>%
mutate(V2 = V2/1000) %>%
mutate(V3 = V3/1000) %>%
rename("Pct Positive TOST" = V2) %>%
rename("Pct Positive T-test" = V3)
sample_needed = as.data.frame(outer_df) %>%
rename("Sample Size" = V1) %>%
mutate(V2 = V2/1000) %>%
mutate(V3 = V3/1000) %>%
rename("Pct Positive TOST" = V2) %>%
rename("Pct Positive T-test" = V3)
sample_needed = as.data.frame(outer_df) %>%
rename("Sample Size" = V1) %>%
mutate(V2 = V2/1000) %>%
mutate(V3 = V3/1000) %>%
rename("Pct Positive TOST" = V2) %>%
rename("Pct Positive T-test" = V3)
pwr.2p2n.test
pwr::pwr.2p2n.test
pwr::pwr.2p2n.test()
pwr::pwr.2p2n.test(h = null, n1 = 96, n2 = 115, sig.level = .05)
pwr::pwr.2p2n.test(h = NULL, n1 = 96, n2 = 115, sig.level = .05)
pwr::pwr.2p2n.test(h = NULL, n1 = 96, n2 = 115, sig.level = .05, power = .8)
powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4),
powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4),
powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4)
pwer::powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4))
pwer::powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4)
pwr::powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4)
powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4),powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4)
TOSTER::powerTOSTtwo(alpha=0.05, statistical_power=0.8, low_eqbound_d=-0.4, high_eqbound_d=0.4)
?powerTOSTtwo
TOSTER::powerTOSTtwo(alpha=0.05, N = 105.5, statistical_power=0.8)
TOSTER::powerTOSTtwo(alpha=0.05, N = 105, statistical_power=0.8)
TOSTER::powerTOSTtwo(alpha=0.05, N = 98, statistical_power=0.8)
TOSTER::powerTOSTtwo(alpha=0.05, N = 105, statistical_power=0.8) # 40
round(mean(c(d_df_i$sd_corr, d_df$sd_corr)), digits = 1)
library("papaja")
library(gtsummary)
source(here::here("scripts", "00_libs.R"))
source(here("scripts", "04_load_data.R"))
source(here("scripts", "05_analysis.R"))
round(mean(c(d_df_i$sd_corr, d_df$sd_corr)), digits = 1)
sd_present = round(mean(c(d_df_i$sd_corr, d_df$sd_corr)), digits = 1)
TOSTER::powerTOSTtwo(alpha=0.05, N = 105, statistical_power=0.8)
sd_present*.0038
sd_present*.0038*20
1.2*20
sqrt(24)
poolsd = sd1 + sd2/2
n = 24
mean1 = 98
sd1 = 2
mean2 = 93
sd2 = 3
poolsd = sd1 + sd2/2
poolsd = (sd1 + sd2)/2
n = 24
mean1 = 98
sd1 = 2
mean2 = 93
sd2 = 3
poolsd = (sd1 + sd2)/2
poolsd/sqrt(n)
poolsd/sqrt(n)*1.96
pwr::pwr.2p2n.test(h = NULL, n1 = 96, n2 = 115, sig.level = .05, power = .8)
pwr::pwr.2p2n.test(h = 2,
n = NULL,
sig.level = .05, power = .8)
?pwer
?pwr
pwr::pwr.p.test(h = 2,
n = NULL,
sig.level = .05, power = .8)
pwr::pwr.p.test(h = 2,
n = NULL,
sig.level = .05, power = .8)
pwr.p.test(h=0.2,n=60,sig.level=0.05,alternative="two.sided")
pwr::pwr.p.test(h=0.2,n=60,sig.level=0.05,alternative="two.sided")
pwr::pwr.p.test(h=2,n=BULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=2,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=2,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=.2,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=1,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=1.5,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=1.8,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=1.9,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=2,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=1.99,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
source("~/.active-rstudio-document", echo=TRUE)
pwr::pwr.p.test(h=1.9,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
pwr::pwr.p.test(h=1.93,n=NULL,sig.level=0.05,power = .8, alternative="two.sided")
# ------------------------------------------------------
# Date 11/9/23
# This script runs the analyses reported in the manuscript
# ANOVA, TOST
# -------------------------------------------------------
## Load Libraries and data.
source(here::here("scripts", "00_libs.R"))
source(here("scripts", "04_load_data.R"))
View(collocation_task)
glimpse(collocation_task)
collocation_task %>%
group_by(prolific_id) %>%
summarize(eng = mean(lextale_score_eng_std),
span = mean(lextale_score_span_std),
port = mean(lextale_score_port_std))
collocation_task %>%
group_by(prolific_id, L1) %>%
summarize(eng = mean(lextale_score_eng_std),
span = mean(lextale_score_span_std),
port = mean(lextale_score_port_std))
collocation_task %>%
group_by(prolific_id, L1) %>%
summarize(eng = mean(lextale_score_eng_std),
span = mean(lextale_score_span_std),
port = mean(lextale_score_port_std)) %>%
pivot_longer(cols = c(eng:port), names_to = "language",
values_to = "score")
## Lextale
proficiency_df = collocation_task %>%
group_by(prolific_id, L1) %>%
summarize(eng = mean(lextale_score_eng_std),
span = mean(lextale_score_span_std),
port = mean(lextale_score_port_std)) %>%
pivot_longer(cols = c(eng:port), names_to = "language",
values_to = "score")
glimpse(proficiency_df)
p_mod = brms::brm(score ~ L1*language + (language | prolific_id), data = proficiency_df)
p_mod = brms::brm(score ~ L1*language + (1 | prolific_id), data = proficiency_df)
conditional_effects(p_mod)
View(proficiency_df)
View(collocation_task)
ldf = read.csv("data", "tidy", "lextale_df.csv")
)
ldf = read.csv(here("data", "tidy", "lextale_df.csv"))
View(ldf)
proficiency_df %>% filter(L1 == "English")
proficiency_df %>% filter(language == "English")
proficiency_df %>% filter(language == "English")
proficiency_df %>% filter(language == "eng")
t.test(score ~ L1 data = e_df)
?t.test
t.test(score ~ L1, data = e_df)
t.test(score ~ L1, data = e_df)
e_df = proficiency_df %>% filter(language == "eng")
t.test(score ~ L1, data = e_df)
e_df = proficiency_df %>% filter(language == "eng" & L1 == "English")
s_df = proficiency_df %>% filter(language == "eng" & L1 == "Spanish")
e_df$score
#### T-test of L1 English grou
t.test(e_df$score, s_df$score)
t.test(score ~ L1, data = e_df)
eng_prof_t_test = t.test(score ~ L1, data = e_df)
e_df = proficiency_df %>% filter(language == "eng")
eng_prof_t_test = t.test(score ~ L1, data = e_df)
library(effectsize)
cohens_d(score ~ L1, data = e_df)
## Tidy a dataframe for analysis
proficiency_df = collocation_task %>%
group_by(prolific_id, L1) %>%
summarize(eng = mean(lextale_score_eng),
span = mean(lextale_score_span_std),
port = mean(lextale_score_port_std)) %>%
pivot_longer(cols = c(eng:port), names_to = "language",
values_to = "score")
## L1 English speakers are more proficient than in their L1
e_df = proficiency_df %>% filter(language == "eng")
t.test(score ~ L1, data = e_df)
eng_prof_t_test = t.test(score ~ L1, data = e_df)
cohens_d(score ~ L1, data = e_df)
## Tidy a dataframe for analysis
proficiency_df = collocation_task %>%
group_by(prolific_id, L1) %>%
summarize(eng = mean(lextale_score_eng_std),
span = mean(lextale_score_span_std),
port = mean(lextale_score_port_std)) %>%
pivot_longer(cols = c(eng:port), names_to = "language",
values_to = "score")
## L1 English speakers are more proficient than in their L1
e_df = proficiency_df %>% filter(language == "eng")
eng_prof_t_test = t.test(score ~ L1, data = e_df)
cohens_d(score ~ L1, data = e_df)
eng_prof_t_test = t.test(score ~ L1, data = e_df)
es_eng_prof_t_test = cohens_d(score ~ L1, data = e_df)
source(here("scripts", "07_analysis_reviews.R"))
es_eng_prof_t_test
es_eng_prof_t_test$Cohens_d
round(es_eng_prof_t_test$Cohens_d, digits = 2)
round(es_eng_prof_t_test$Cohens_d, digits = 2)
round(es_eng_prof_t_test$CI_low, digits = 2)
round(es_eng_prof_t_test$CI digits = 2)
round(es_eng_prof_t_test$CI, digits = 2)
round(es_eng_prof_t_test$CI_low, digits = 2)
round(es_eng_prof_t_test$CI_high, digits = 2)
eng_prof_t_test$statistic
round(eng_prof_t_test$statistic, digits = 2)
round(eng_prof_t_test$p.value, digits = 2)
round(eng_prof_t_test$parameter, digits = 2)
s_df = proficiency_df %>% filter(language == "span")
t.test(score ~ L1, data = s_df)
cohens_d(score ~ L1, data = s_df)
e_in_df = proficiency_df %>% filter(L1 == "English") %>%
filter(lang == "span" | lang == "port")
e_in_df = proficiency_df %>% filter(L1 == "English") %>%
filter(language == "span" | language == "port")
View(e_in_df)
t.test(score ~ language, paired = TRUE, data = e_in_df)
View(ldf)
# ------------------------------------------------------
# Date 11/9/23
# This script loads the already tidy data for analysis and reporting
#
# -------------------------------------------------------
## Adding analysis for reporting
res = TOSTER::powerTOSTtwo(alpha=0.05, N=15, statistical_power=0.8)
## Load Collocation Task
collocation_task = read.csv(here("data", "tidy", "collocation_task.csv")) %>%
filter(L1 == "English" | L1 == "Spanish") %>%
mutate(lextale_score_eng_std = (lextale_score_eng - mean(lextale_score_eng)) / sd(lextale_score_eng),
lextale_score_span_std = (lextale_score_span - mean(lextale_score_span)) / sd(lextale_score_span),
lextale_score_port_std = (lextale_score_port - mean(lextale_score_port)) / sd(lextale_score_port))
## Load Interpretation Task
interpretation_task = read.csv(here("data", "tidy", "interpretation_task.csv")) %>%
filter(L1 == "English" | L1 == "Spanish") %>%
filter(!is.na(L1)) %>%
mutate(lextale_score_eng_std = (lextale_score_eng - mean(lextale_score_eng)) / sd(lextale_score_eng),
lextale_score_span_std = (lextale_score_span - mean(lextale_score_span)) / sd(lextale_score_span),
lextale_score_port_std = (lextale_score_port - mean(lextale_score_port)) / sd(lextale_score_port))
## Tidy the results of the Collocation task for analyis
aov_df_col = collocation_task %>%
group_by(pre_post, gorilla_id, L1) %>%
summarize(correct_no_coll = sum(is_correct)) %>%
pivot_wider(names_from = "pre_post", values_from = "correct_no_coll") %>%
rename("pre_coll" = "pre",
"post_coll" = "post") %>%
filter(L1 == "English" | L1 == "Spanish") %>%
pivot_longer(cols = c("post_coll", "pre_coll"), names_to = "pre_post", values_to = "n_correct")
## Tidy the results of the Interpreratio task for analyis
aov_df_int = interpretation_task %>%
group_by(pre_post, gorilla_id, L1) %>%
summarize(correct_no_coll = sum(is_correct)) %>%
pivot_wider(names_from = "pre_post", values_from = "correct_no_coll") %>%
rename("pre_coll" = "pre",
"post_coll" = "post") %>%
filter(L1 == "English" | L1 == "Spanish") %>%
pivot_longer(cols = c("post_coll", "pre_coll"), names_to = "pre_post", values_to = "n_correct")
## Tidy how many participants completed the tasks
tot_df = interpretation_task %>%
group_by(prolific_id, L1) %>%
summarize(n = n())  %>%
group_by(L1) %>%
summarize(n = n())
## Tidy to report age of acquisition
aoa_df = collocation_task %>%
group_by(prolific_id, L1) %>%
summarise(English = mean(eng_aoa),
Spanish = mean(span_aoa),
Portuguese = mean(port_aoa)) %>%
pivot_longer(cols = c("English", "Spanish", "Portuguese"),
names_to = "Language",
values_to = "Score")
## Tidy to report age of acquisition
rep_aoa = aoa_df %>%
group_by(L1, Language) %>%
summarise(mean_aoa = round(mean(Score), digits = 2),
sd_aoa = round(sd(Score), digits = 1))
## Load Libraries and data.
source(here::here("scripts", "00_libs.R"))
source(here("scripts", "04_load_data.R"))
## Tidy a dataframe for analysis
proficiency_df = collocation_task %>%
group_by(prolific_id, L1) %>%
summarize(eng = mean(lextale_score_eng_std),
span = mean(lextale_score_span_std),
port = mean(lextale_score_port_std)) %>%
pivot_longer(cols = c(eng:port), names_to = "language",
values_to = "score")
## L1 English speakers are more proficient than in their L1
e_df = proficiency_df %>% filter(language == "eng")
eng_prof_t_test = t.test(score ~ L1, data = e_df)
es_eng_prof_t_test = cohens_d(score ~ L1, data = e_df)
## L1 Spanish speakers are more proficient than in their L1
s_df = proficiency_df %>% filter(language == "span")
span_prof_t_test = t.test(score ~ L1, data = s_df)
es_span_prof_t_test = cohens_d(score ~ L1, data = s_df)
## The English L1 groups in the L2 and L3
e_in_df = proficiency_df %>% filter(L1 == "English") %>%
filter(language == "span" | language == "port")
t.test(score ~ language, paired = TRUE, data = e_in_df)
cohens_d(score ~ L1, data = e_in_df)
cohens_d(score ~ L1, data = e_in_df)
cohens_d(score ~ language, data = e_in_df)
s_in_df = proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
s_l2_l3_t_test = t.test(score ~ language, paired = TRUE, data = s_in_df)
es_s_l2_l23 = cohens_d(score ~ language, data = s_in_df)
t.test(score ~ language, paired = TRUE, data = s_in_df)
cohens_d(score ~ language, data = s_in_df)
round(es_e_l2_l23$Cohens_d, digits = 2)
round(es_e_l2_l23$Cohens_d, digits = 2)
e_l2_l3_t_test = t.test(score ~ language, paired = TRUE, data = e_in_df)
es_e_l2_l23 = cohens_d(score ~ language, data = e_in_df)
round(es_e_l2_l23$Cohens_d, digits = 2)
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" | language == "port")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
proficiency_df %>% filter(L1 == "Spanish") %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
l2_comp = proficiency_df %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
l2_comp = proficiency_df %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
View(l2_comp)
l2_comp_t_test = t.test(score ~ L1, paired = TRUE, data = s_in_df)
l2_comp = proficiency_df %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
l2_comp_t_test = t.test(score ~ L1, paired = TRUE, data = s_in_df)
l2_comp = proficiency_df %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
l2_comp_t_test = t.test(score ~ L1, paired = TRUE, data = l2_comp)
l2_comp = proficiency_df %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
l2_comp_t_test = t.test(score ~ L1, paired = TRUE, data = l2_comp)
l2_comp = proficiency_df %>%
filter(language == "eng" & L1 == "Spanish" | language == "span" & L1 == "English")
l2_comp_t_test = t.test(score ~ L1, data = l2_comp)
es_l2_comp = cohens_d(score ~ L1, data = l2_comp)
t.test(score ~ L1, data = l2_comp)
cohens_d(score ~ L1, data = l2_comp
)
# Compare L3s
l3_comp = proficiency_df %>%
filter(language == "port")
View(l3_comp)
l3_comp_t_test = t.test(score ~ L1, data = l3_comp)
es_l3_comp = cohens_d(score ~ L1, data = l3_comp)
t.test(score ~ L1, data = l3_comp)
cohens_d(score ~ L1, data = l3_comp)
glimpse(collocation_task)
aoa_df = collocation_task %>%
group_by(prolific_id, L1) %>%
summarize(eng = mean(eng_aoa),
span = mean(span_aoa),
port = mean(port_aoa))
View(aoa_df)
aoa_df = collocation_task %>%
group_by(prolific_id, L1) %>%
summarize(eng = mean(eng_aoa),
span = mean(span_aoa),
port = mean(port_aoa))  %>%
pivot_longer(cols = c(eng:port), names_to = "language",
values_to = "score")
aoa_eng_comp = aoa_df %>% filter(L1 == "English")
aoa_eng_comp = aoa_df %>% filter(L1 == "English" & lang =! "port")
aoa_eng_comp = aoa_df %>% filter(L1 == "English" & language != "port")
View(aoa_eng_comp)
aoa_eng_comp = aoa_df %>% filter(L1 == "English" & language != "port")
aoa_e_c_t = t.test(score ~ L1, data = aoa_eng_comp)
aoa_e_c_t = t.test(score ~ language, data = aoa_eng_comp)
es_aoa_e_c_t = cohens_d(score ~ language, data = aoa_eng_comp)
t.test(score ~ language, data = aoa_eng_comp)
cohens_d(score ~ language, data = aoa_eng_comp)
cohens_d(score ~ language, data = aoa_eng_comp)
t.test(score ~ language, data = aoa_eng_comp)
aoa_eng_comp
aoa_e_c_t
aoa_eng_comp = aoa_df %>% filter(L1 == "English" & language != "eng")
aoa_e_c_t = t.test(score ~ language, data = aoa_eng_comp)
es_aoa_e_c_t = cohens_d(score ~ language, data = aoa_eng_comp)
t.test(score ~ language, data = aoa_eng_comp)
cohens_d(score ~ language, data = aoa_eng_comp)
aoa_span_comp = aoa_df %>% filter(L1 == "English" & language != "span")
aoa_s_c_t = t.test(score ~ language, data = aoa_span_comp)
es_aoa_s_c_t = cohens_d(score ~ language, data = aoa_span_comp)
t.test(score ~ language, data = aoa_span_comp)
cohens_d(score ~ language, data = aoa_span_comp)
aoa_span_comp = aoa_df %>% filter(L1 == "Spanish" & language != "span")
aoa_s_c_t = t.test(score ~ language, data = aoa_span_comp)
es_aoa_s_c_t = cohens_d(score ~ language, data = aoa_span_comp)
t.test(score ~ language, data = aoa_span_comp)
cohens_d(score ~ language, data = aoa_span_comp)
View(es_aoa_e_c_t)
install.packages("latexpdf")
library("papaja")
library(gtsummary)
source(here::here("scripts", "00_libs.R"))
source(here("scripts", "04_load_data.R"))
source(here("scripts", "05_analysis.R"))
source(here("scripts", "07_analysis_reviews.R"))
knitr::include_graphics(here("docs", "figs", "prof_plot.png"))
#| fig.width = 10
knitr::include_graphics(here("docs", "figs", "prof_plot.png"))
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(
comment = '', out.width = "400px"))
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(
comment = '', out.width = "400px")
knitr::include_graphics(here("docs", "figs", "pc_t.png"))
.15/1,1
.15/1.1
